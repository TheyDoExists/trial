---
title: "Week 3 Assignment"
format: 
  html:
    embed-resources: true
editor: visual
---

## Week 3 Assignment

## Return Prediction: Brief Explanation

In Lab3, we will use the stock return data. We have monthly excess return data on a diversified portfolio along with 5 factors that are shown in the literature to have an impact on stock returns. Nobel Laureate Eugene Fama and Kenneth French originally introduced the three factors model in their 1993 Journal of Financial Economics article. Then, 2 more factors were added to the original model, called the 5-factor model. In this lab assignment, your task is to apply model selection techniques, knn, and cross-validation methods to come up with a model that will be used to estimate stock returns in new data set and the test root mean squared error (RMSE).

-   **TARGET VARIABLE:** Excess return on a diversified portfolio and it is captured as return on a portfolio - risk free rate (return on Long-term US Government Bond returns).

-   **5-Factors:**

    1\. SIZE:Small-cap stocks tend to outperform large-cap stocks (Size is measured by stock price \* shares outstanding)

    2\. VALUE: Cheaper stocks (Value stocks) tend to outperform expensive (Growth) stocks (Inexpensiveness: Book Value/Market Value, Book to Market ratio, B/M)

    \- Lower the B/M, expensive the stock (Growth stocks)

    \- Higher the B/M, cheap the stock (Value Stocks)

    3\. MOMENTUM: Winners outperform losers

    4\. RISK (BETA): Lower the beta of a stock, higher the return performance

    5\. QUALITY: Higher the profitability, higher the return performance

## Data Dictionary

-   We have 500 observations in the original data, called Full_data. Data spans from November 1976 till June 2018.

-   We divided Full_data into two sets: first400 and testset. The first 400 monthly observations were kept for training and validation purposes. Monthly data from November 1976 till February 2010 were randomly divided into two groups. You can use the trainingset to train alternative models and validationset to check your model performance.

-   **testset**: The last 100 monthly observations are kept as our testing data and it spans from March 2010 till June 2018.

**Target Variable**

**Y**: Excess return on a portfolio= Portolio return - risk free rate (return on US Government bonds)

**Factors (Predictors)**

1\. SMB to capture size

2\. HML to capture Value

3\. MOM to capture Momentum

4\. BAB to capture Risk

5\. QMJ to capture Quality

6\. MRP: A measure of average market risk premium: measures as return on a value-weighted market portfolio - risk free rate.

Run the following R chunk code before working on the questions.

```{r, echo=FALSE}
# WARNING: Do not modify the codes in here. 
# Run this code before moving to the next one

library(PerformanceAnalytics)
library(xts)
library(lubridate)
library(tidyverse)
library(dplyr)
library(caret)
library(e1071)
library(class)
library(ggplot2)
library(Metrics)


my_factors <- read.csv("Data_RLab3.csv") # call the data
my_factors$Date <- mdy(my_factors$Date) # declare the date variable
my_factors_sorted<- my_factors[order(my_factors$Date),] # sort by date
All_data <- xts(my_factors_sorted[,-1],order.by = my_factors_sorted[,1],)
All_data$Y<-All_data$Brk_ret-All_data$RF  # target variable

Full_data<-as.data.frame(All_data) # convert to data frame
Fulldata = subset(Full_data, select = -c(RF,Brk_ret,Brk_exret,Subperiod, Mkt))# drop redundant ones
Fulldata<-Fulldata%>%
    rename(MRP=Mkt_rf, MOM=Mom)

first400<-Fulldata[1:400,]  # use the first 400 as training and validation set
testset<-Fulldata[401:500,]  # last 100 for the test set

set.seed(5410)   # use this seed
# shuffle the index for the testing data
shuffle<-sample(nrow(first400), 0.25*nrow(first400))
 # Get the training data in training set
trainingset<-first400[-shuffle,]
# Get the validation set in trainingf  data
validationset<-first400[shuffle,]


```

## PART I

In this part, you will be totally blind to **testset** (You can't use **testset** in part I).

#### Part I: Question 1

Use the step function in stats package and run a forward stepwise regression on **trainingset** and name your model **model_forward**. If we use AIC information criteria, which variables are selected based on model_forward?

```{r, echo=TRUE}
# HINT: use step function and choose either criterion = "AIC" and criterion = "BIC"
# Enter your code below

minM <- lm(Y~ 1, data=trainingset)
maxM<-lm(Y~. , data=trainingset)


#model_forward <- step(lm(Y ~ ., data = trainingset), direction = "forward", trace = FALSE, k = log(nrow(trainingset))) # I dont understand how this line of code is the same as the following line of code.


model_forward <-stats::step(minM ,direction= "forward", scope=formula(maxM), criterion = "AIC") #is using the stepwise regression method to build a linear                          regression model.   

summary(model_forward)

```

```{r}
# performs a backward stepwise regression using the BIC criterion. It starts with the full model (maxM) and removes one predictor at a time, checking if the model fit improves according to the BIC criterion.

#Model_backward <-stats::step(maxM,direction="backward", scope=formula(maxM),criterion ="BIC")

#summary(Model_backward)
```

#### Part I: Question 2

Use the step function in stats package and run a backward stepwise regression on **trainingset** and name your model **model_backward**. If we use **BIC** information criteria, which variables are selected based on **model_backward**?

```{r, echo=TRUE}
# HINT: use step function and choose either criterion = "AIC" and criterion = "BIC"
# Enter your code below

# performs a backward stepwise regression using the BIC criterion. It starts with the full model (maxM) and removes one predictor at a time, checking if the model fit improves according to the BIC criterion.

model_backward <-stats::step(maxM,direction="backward", scope=formula(maxM),criterion ="BIC")

summary(model_backward)

```

#### Part I: Question 3

Fit **model_forward** and **model_backward** models on **validationset** data and calculate the corresponding **RMSE** values.

```{r, echo=TRUE}
# HINT 1: Use predict () function to get the predictions. RMSE formula: sqrt(mean((Actual-Fitted)^2))

# Fit model_forward on validationset and calculate RMSE
pred_forward <- predict(model_forward, newdata = validationset)

#The lower the RMSE, the better the model fits the data.The validationset$Y is referring to the target variable Y in the validation set.
RMSE_forward <- sqrt(mean((pred_forward - validationset$Y)^2))

summary(pred_forward)
print(RMSE_forward)

# HINT 2: An easier way would be to use rmse() function in Metrics package
# Enter your code below





```

```{r}
# Fit model_backward on validationset and calculate RMSE
pred_backward <- predict(model_backward, newdata = validationset)

#The lower the RMSE, the better the model fits the data.The validationset$Y is referring to the target variable Y in the validation set.
RMSE_backward <- sqrt(mean((pred_backward - validationset$Y)^2))

summary(pred_backward)
print(RMSE_backward)


```

## KNN Regression

In this part, you will be totally blind to **testset** (You can't use testset in part II).

In Part II, by using the **caret** package in R, your task is to fit the following five models to the **first400** dataset by using K-nearest neighbors regression (KNN regression) method to find the right value of k for each model.

-   **model1**: $Y=\beta_{0}+\beta_{1}MRP+\beta_{2}SMB+\epsilon$

-   **model5**: $Y=\beta_{0}+\beta_{1}MRP+\beta_{2}SMB+\beta_{3}HML+\beta_{4}MOM+\beta_{5}BAB+\beta_{6}QMJ+\epsilon$

#### PART II Question 4

Use the **train** function in **caret** package, use knn to train **model1** with **first400** data. Use 10-fold cross validation. Use the **set.seed(2022)** seed values and by using expand.grid, evaluate odd k values up to 50. Use scaled and centered data by using the preProcess function and name your model as **knn_model1**.

What is the average RMSE at optimal k value?

```{r, echo=TRUE}
# HINT: Use preProcess=c('center', 'scale') to preprocess the data
#preProcess=c('center', 'scale') 

# HINT: Use preProcess=c('center', 'scale') to preprocess the data
preproc <- list(preProcess = c('center', 'scale'))


# train knn model with first400 data
knn_model1 <- train(Y ~ MRP + SMB, data = first400, method = "knn", preProcess = c('center', 'scale'),
                    tuneGrid = expand.grid(k = seq(1, 50, by = 2)), trControl = trainControl(method = "cv", number = 10))


# HINT: Use tuneGrid = expand.grid(k = seq(1, 50, by = 2)) for odd k grid search
#tuneGrid = expand.grid(k = seq(1, 50, by = 2))


# HINT: Use trControl = trainControl(method = "CV", number = 10) for 10-fold cros validation
trControl <- trainControl(method = "cv", number = 10)



# HINT: knn_model1$results will produce the average results
optimal_k <- knn_model1$bestTune$k
RMSE <- knn_model1$results[knn_model1$results$k == optimal_k, "RMSE"]
RMSE


# Enter your code below


set.seed(2022)




```

#### PART II Question 5

Use the **train** function in **caret** package, use knn to train **model5** with **first400** data. Use 10-fold cross validation. Use the **set.seed(2022)** seed values and by using expand.grid, evaluate odd k values up to 50. Use scaled and centered data by using the preProcess function. Call your model **knn_model5**.

What is the optimal k value?

```{r, echo=TRUE}
# HINT: Use tuneGrid = expand.grid(k = seq(1, 50, by = 2)) for odd k grid search
# HINT: Use preProcess=c('center', 'scale') to preprocess the data
# HINT: Use trControl = trainControl(method = "CV", number = 10) for 10-fold cros validation
# HINT: knn_model5$bestTune will produce the average results
# Enter your code below


set.seed(2022)

# Define preprocessing method
preproc <- c('center', 'scale')

# Train KNN model for model5
knn_model5 <- train(Y ~ MRP + SMB + HML + MOM + BAB + QMJ, data = first400, 
                    method = "knn", preProcess = preproc, 
                    tuneGrid = expand.grid(k = seq(1, 50, by = 2)), 
                    trControl = trainControl(method = "cv", number = 10))

# Find optimal k value
optimal_k <- knn_model5$bestTune$k
optimal_k


```

#### PART II Question 6

Use **knn_model5** to predict Y in **testset** data and name your predictions as **knn_model5_predict**. What is the **RMSE** value in testset based on knn_model5_predict?

```{r, echo=TRUE}
# HINT: use predict function
# HINT: you canuse rmse() function in Metrics package
# Enter your code below

# predict Y in testset using knn_model5
knn_model5_predict <- predict(knn_model5, newdata = testset)

# calculate RMSE value in testset
RMSE <- sqrt(mean((testset$Y - knn_model5_predict)^2))

# print RMSE value
RMSE
```

#### PART III Question 7

If we define best model as the one with lowest RMSE value in \*\***testset**\*\*, which of the following is your best model?

```{r, echo=TRUE}
# HINT: use predict() function to get the predictions on testset
# HINT: you canuse rmse() function in Metrics package to calculate RMSE values
# Enter your code below

# Predictions for knn_model1
knn_model1_predict <- predict(knn_model1, newdata = testset)
rmse(testset$Y, knn_model1_predict)

# Predictions for knn_model5
knn_model5_predict <- predict(knn_model5, newdata = testset)
rmse(testset$Y, knn_model5_predict)

# Predictions for model_forward
model_forward_predict <- predict(model_forward, newdata = testset)
rmse(testset$Y,model_forward_predict )

# Predictions for model_backward
model_backward_predict <- predict(model_backward, newdata = testset)
rmse(testset$Y,model_backward_predict )
 
#library(Metrics)
RMSE_knn1 <- rmse(predict(knn_model1, newdata = testset), testset$Y)
RMSE_knn5 <- rmse(knn_model5_predict, testset$Y)
RMSE_backward <- rmse(model_backward_predict, testset$Y )
RMSE_forward <- rmse(model_forward_predict, testset$Y )

# print(paste("The value of my object is", my_object))
print(paste("The value of RMSE_knn1 = ", RMSE_knn1))
print(paste("The value of RMSE_knn5 = ", RMSE_knn5))
print(paste("The value of RMSE_backward = ", RMSE_backward))
print(paste("The value of RMSE_forward = ", RMSE_forward))

```

#### Question 8

Click on Render icon on to convert this file into HTML format before submitting in Canvas
