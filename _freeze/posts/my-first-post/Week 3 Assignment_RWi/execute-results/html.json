{
  "hash": "3653fff6b4ada0c49fb2668379dcaf3c",
  "result": {
    "markdown": "---\ntitle: \"Week 3 Assignment\"\nformat: \n  html:\n    embed-resources: true\neditor: visual\n---\n\n\n## Week 3 Assignment\n\n## Return Prediction: Brief Explanation\n\nIn Lab3, we will use the stock return data. We have monthly excess return data on a diversified portfolio along with 5 factors that are shown in the literature to have an impact on stock returns. Nobel Laureate Eugene Fama and Kenneth French originally introduced the three factors model in their 1993 Journal of Financial Economics article. Then, 2 more factors were added to the original model, called the 5-factor model. In this lab assignment, your task is to apply model selection techniques, knn, and cross-validation methods to come up with a model that will be used to estimate stock returns in new data set and the test root mean squared error (RMSE).\n\n-   **TARGET VARIABLE:** Excess return on a diversified portfolio and it is captured as return on a portfolio - risk free rate (return on Long-term US Government Bond returns).\n\n-   **5-Factors:**\n\n    1\\. SIZE:Small-cap stocks tend to outperform large-cap stocks (Size is measured by stock price \\* shares outstanding)\n\n    2\\. VALUE: Cheaper stocks (Value stocks) tend to outperform expensive (Growth) stocks (Inexpensiveness: Book Value/Market Value, Book to Market ratio, B/M)\n\n    \\- Lower the B/M, expensive the stock (Growth stocks)\n\n    \\- Higher the B/M, cheap the stock (Value Stocks)\n\n    3\\. MOMENTUM: Winners outperform losers\n\n    4\\. RISK (BETA): Lower the beta of a stock, higher the return performance\n\n    5\\. QUALITY: Higher the profitability, higher the return performance\n\n## Data Dictionary\n\n-   We have 500 observations in the original data, called Full_data. Data spans from November 1976 till June 2018.\n\n-   We divided Full_data into two sets: first400 and testset. The first 400 monthly observations were kept for training and validation purposes. Monthly data from November 1976 till February 2010 were randomly divided into two groups. You can use the trainingset to train alternative models and validationset to check your model performance.\n\n-   **testset**: The last 100 monthly observations are kept as our testing data and it spans from March 2010 till June 2018.\n\n**Target Variable**\n\n**Y**: Excess return on a portfolio= Portolio return - risk free rate (return on US Government bonds)\n\n**Factors (Predictors)**\n\n1\\. SMB to capture size\n\n2\\. HML to capture Value\n\n3\\. MOM to capture Momentum\n\n4\\. BAB to capture Risk\n\n5\\. QMJ to capture Quality\n\n6\\. MRP: A measure of average market risk premium: measures as return on a value-weighted market portfolio - risk free rate.\n\nRun the following R chunk code before working on the questions.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: xts\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: zoo\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'zoo'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\n################################### WARNING ###################################\n# We noticed you have dplyr installed. The dplyr lag() function breaks how    #\n# base R's lag() function is supposed to work, which breaks lag(my_xts).      #\n#                                                                             #\n# If you call library(dplyr) later in this session, then calls to lag(my_xts) #\n# that you enter or source() into this session won't work correctly.          #\n#                                                                             #\n# All package code is unaffected because it is protected by the R namespace   #\n# mechanism.                                                                  #\n#                                                                             #\n# Set `options(xts.warn_dplyr_breaks_lag = FALSE)` to suppress this warning.  #\n#                                                                             #\n# You can use stats::lag() to make sure you're not using dplyr::lag(), or you #\n# can add conflictRules('dplyr', exclude = 'lag') to your .Rprofile to stop   #\n# dplyr from breaking base R's lag() function.                                #\n################################### WARNING ###################################\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'PerformanceAnalytics'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:graphics':\n\n    legend\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'lubridate'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr   1.1.1     ✔ readr   2.1.4\n✔ forcats 1.0.0     ✔ stringr 1.5.0\n✔ ggplot2 3.4.1     ✔ tibble  3.2.1\n✔ purrr   1.0.1     ✔ tidyr   1.3.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::first()  masks xts::first()\n✖ dplyr::lag()    masks stats::lag()\n✖ dplyr::last()   masks xts::last()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\nLoading required package: lattice\n\n\nAttaching package: 'caret'\n\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n\n\nAttaching package: 'e1071'\n\n\nThe following objects are masked from 'package:PerformanceAnalytics':\n\n    kurtosis, skewness\n\n\n\nAttaching package: 'Metrics'\n\n\nThe following objects are masked from 'package:caret':\n\n    precision, recall\n```\n:::\n:::\n\n\n## PART I\n\nIn this part, you will be totally blind to **testset** (You can't use **testset** in part I).\n\n#### Part I: Question 1\n\nUse the step function in stats package and run a forward stepwise regression on **trainingset** and name your model **model_forward**. If we use AIC information criteria, which variables are selected based on model_forward?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# HINT: use step function and choose either criterion = \"AIC\" and criterion = \"BIC\"\n# Enter your code below\n\nminM <- lm(Y~ 1, data=trainingset)\nmaxM<-lm(Y~. , data=trainingset)\n\n\n#model_forward <- step(lm(Y ~ ., data = trainingset), direction = \"forward\", trace = FALSE, k = log(nrow(trainingset))) # I dont understand how this line of code is the same as the following line of code.\n\n\nmodel_forward <-stats::step(minM ,direction= \"forward\", scope=formula(maxM), criterion = \"AIC\") #is using the stepwise regression method to build a linear                          regression model.   \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStart:  AIC=-1558.46\nY ~ 1\n\n       Df Sum of Sq    RSS     AIC\n+ MRP   1  0.235984 1.4164 -1602.7\n+ SMB   1  0.034658 1.6178 -1562.8\n+ BAB   1  0.021256 1.6312 -1560.3\n+ HML   1  0.014158 1.6383 -1559.0\n<none>              1.6524 -1558.5\n+ MOM   1  0.002952 1.6495 -1557.0\n+ QMJ   1  0.002932 1.6495 -1557.0\n\nStep:  AIC=-1602.69\nY ~ MRP\n\n       Df Sum of Sq    RSS     AIC\n+ HML   1  0.094763 1.3217 -1621.5\n+ SMB   1  0.069893 1.3465 -1615.9\n+ BAB   1  0.058510 1.3579 -1613.3\n+ QMJ   1  0.045470 1.3710 -1610.5\n<none>              1.4164 -1602.7\n+ MOM   1  0.001195 1.4152 -1600.9\n\nStep:  AIC=-1621.46\nY ~ MRP + HML\n\n       Df Sum of Sq    RSS     AIC\n+ QMJ   1  0.065389 1.2563 -1634.7\n+ SMB   1  0.036634 1.2851 -1627.9\n+ BAB   1  0.019178 1.3025 -1623.8\n<none>              1.3217 -1621.5\n+ MOM   1  0.001043 1.3206 -1619.7\n\nStep:  AIC=-1634.68\nY ~ MRP + HML + QMJ\n\n       Df Sum of Sq    RSS     AIC\n<none>              1.2563 -1634.7\n+ SMB   1 0.0065364 1.2498 -1634.2\n+ BAB   1 0.0051438 1.2511 -1633.9\n+ MOM   1 0.0004749 1.2558 -1632.8\n```\n:::\n\n```{.r .cell-code}\nsummary(model_forward)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Y ~ MRP + HML + QMJ, data = trainingset)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.149298 -0.040926 -0.009752  0.029454  0.314162 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.008428   0.004057   2.078 0.038617 *  \nMRP         1.022721   0.108353   9.439  < 2e-16 ***\nHML         0.696876   0.134062   5.198 3.76e-07 ***\nQMJ         0.762569   0.194279   3.925 0.000108 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.06515 on 296 degrees of freedom\nMultiple R-squared:  0.2397,\tAdjusted R-squared:  0.232 \nF-statistic: 31.11 on 3 and 296 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# performs a backward stepwise regression using the BIC criterion. It starts with the full model (maxM) and removes one predictor at a time, checking if the model fit improves according to the BIC criterion.\n\n#Model_backward <-stats::step(maxM,direction=\"backward\", scope=formula(maxM),criterion =\"BIC\")\n\n#summary(Model_backward)\n```\n:::\n\n\n#### Part I: Question 2\n\nUse the step function in stats package and run a backward stepwise regression on **trainingset** and name your model **model_backward**. If we use **BIC** information criteria, which variables are selected based on **model_backward**?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# HINT: use step function and choose either criterion = \"AIC\" and criterion = \"BIC\"\n# Enter your code below\n\n# performs a backward stepwise regression using the BIC criterion. It starts with the full model (maxM) and removes one predictor at a time, checking if the model fit improves according to the BIC criterion.\n\nmodel_backward <-stats::step(maxM,direction=\"backward\", scope=formula(maxM),criterion =\"BIC\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStart:  AIC=-1632.48\nY ~ MOM + MRP + SMB + HML + QMJ + BAB\n\n       Df Sum of Sq    RSS     AIC\n- MOM   1   0.00042 1.2409 -1634.4\n<none>              1.2405 -1632.5\n- SMB   1   0.00901 1.2495 -1632.3\n- BAB   1   0.00924 1.2497 -1632.2\n- QMJ   1   0.02075 1.2613 -1629.5\n- HML   1   0.04302 1.2835 -1624.2\n- MRP   1   0.32060 1.5611 -1565.5\n\nStep:  AIC=-1634.38\nY ~ MRP + SMB + HML + QMJ + BAB\n\n       Df Sum of Sq    RSS     AIC\n<none>              1.2409 -1634.4\n- BAB   1   0.00883 1.2498 -1634.2\n- SMB   1   0.01022 1.2511 -1633.9\n- QMJ   1   0.02049 1.2614 -1631.5\n- HML   1   0.04601 1.2869 -1625.5\n- MRP   1   0.32019 1.5611 -1567.5\n```\n:::\n\n```{.r .cell-code}\nsummary(model_backward)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Y ~ MRP + SMB + HML + QMJ + BAB, data = trainingset)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.15134 -0.04109 -0.00961  0.02867  0.31797 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.009423   0.004214   2.236  0.02611 *  \nMRP          0.971536   0.111547   8.710 2.25e-16 ***\nSMB         -0.220807   0.141900  -1.556  0.12077    \nHML          0.529265   0.160313   3.301  0.00108 ** \nQMJ          0.516003   0.234190   2.203  0.02835 *  \nBAB          0.171527   0.118607   1.446  0.14919    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.06497 on 294 degrees of freedom\nMultiple R-squared:  0.249,\tAdjusted R-squared:  0.2363 \nF-statistic:  19.5 on 5 and 294 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n#### Part I: Question 3\n\nFit **model_forward** and **model_backward** models on **validationset** data and calculate the corresponding **RMSE** values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# HINT 1: Use predict () function to get the predictions. RMSE formula: sqrt(mean((Actual-Fitted)^2))\n\n# Fit model_forward on validationset and calculate RMSE\npred_forward <- predict(model_forward, newdata = validationset)\n\n#The lower the RMSE, the better the model fits the data.The validationset$Y is referring to the target variable Y in the validation set.\nRMSE_forward <- sqrt(mean((pred_forward - validationset$Y)^2))\n\nsummary(pred_forward)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-0.161152 -0.003284  0.020652  0.018176  0.042518  0.098228 \n```\n:::\n\n```{.r .cell-code}\nprint(RMSE_forward)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.05095637\n```\n:::\n\n```{.r .cell-code}\n# HINT 2: An easier way would be to use rmse() function in Metrics package\n# Enter your code below\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit model_backward on validationset and calculate RMSE\npred_backward <- predict(model_backward, newdata = validationset)\n\n#The lower the RMSE, the better the model fits the data.The validationset$Y is referring to the target variable Y in the validation set.\nRMSE_backward <- sqrt(mean((pred_backward - validationset$Y)^2))\n\nsummary(pred_backward)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \n-0.1711708  0.0001039  0.0225263  0.0179283  0.0428137  0.0936176 \n```\n:::\n\n```{.r .cell-code}\nprint(RMSE_backward)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.05007593\n```\n:::\n:::\n\n\n## KNN Regression\n\nIn this part, you will be totally blind to **testset** (You can't use testset in part II).\n\nIn Part II, by using the **caret** package in R, your task is to fit the following five models to the **first400** dataset by using K-nearest neighbors regression (KNN regression) method to find the right value of k for each model.\n\n-   **model1**: $Y=\\beta_{0}+\\beta_{1}MRP+\\beta_{2}SMB+\\epsilon$\n\n-   **model5**: $Y=\\beta_{0}+\\beta_{1}MRP+\\beta_{2}SMB+\\beta_{3}HML+\\beta_{4}MOM+\\beta_{5}BAB+\\beta_{6}QMJ+\\epsilon$\n\n#### PART II Question 4\n\nUse the **train** function in **caret** package, use knn to train **model1** with **first400** data. Use 10-fold cross validation. Use the **set.seed(2022)** seed values and by using expand.grid, evaluate odd k values up to 50. Use scaled and centered data by using the preProcess function and name your model as **knn_model1**.\n\nWhat is the average RMSE at optimal k value?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# HINT: Use preProcess=c('center', 'scale') to preprocess the data\n#preProcess=c('center', 'scale') \n\n# HINT: Use preProcess=c('center', 'scale') to preprocess the data\npreproc <- list(preProcess = c('center', 'scale'))\n\n\n# train knn model with first400 data\nknn_model1 <- train(Y ~ MRP + SMB, data = first400, method = \"knn\", preProcess = c('center', 'scale'),\n                    tuneGrid = expand.grid(k = seq(1, 50, by = 2)), trControl = trainControl(method = \"cv\", number = 10))\n\n\n# HINT: Use tuneGrid = expand.grid(k = seq(1, 50, by = 2)) for odd k grid search\n#tuneGrid = expand.grid(k = seq(1, 50, by = 2))\n\n\n# HINT: Use trControl = trainControl(method = \"CV\", number = 10) for 10-fold cros validation\ntrControl <- trainControl(method = \"cv\", number = 10)\n\n\n\n# HINT: knn_model1$results will produce the average results\noptimal_k <- knn_model1$bestTune$k\nRMSE <- knn_model1$results[knn_model1$results$k == optimal_k, \"RMSE\"]\nRMSE\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.065852\n```\n:::\n\n```{.r .cell-code}\n# Enter your code below\n\n\nset.seed(2022)\n```\n:::\n\n\n#### PART II Question 5\n\nUse the **train** function in **caret** package, use knn to train **model5** with **first400** data. Use 10-fold cross validation. Use the **set.seed(2022)** seed values and by using expand.grid, evaluate odd k values up to 50. Use scaled and centered data by using the preProcess function. Call your model **knn_model5**.\n\nWhat is the optimal k value?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# HINT: Use tuneGrid = expand.grid(k = seq(1, 50, by = 2)) for odd k grid search\n# HINT: Use preProcess=c('center', 'scale') to preprocess the data\n# HINT: Use trControl = trainControl(method = \"CV\", number = 10) for 10-fold cros validation\n# HINT: knn_model5$bestTune will produce the average results\n# Enter your code below\n\n\nset.seed(2022)\n\n# Define preprocessing method\npreproc <- c('center', 'scale')\n\n# Train KNN model for model5\nknn_model5 <- train(Y ~ MRP + SMB + HML + MOM + BAB + QMJ, data = first400, \n                    method = \"knn\", preProcess = preproc, \n                    tuneGrid = expand.grid(k = seq(1, 50, by = 2)), \n                    trControl = trainControl(method = \"cv\", number = 10))\n\n# Find optimal k value\noptimal_k <- knn_model5$bestTune$k\noptimal_k\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 17\n```\n:::\n:::\n\n\n#### PART II Question 6\n\nUse **knn_model5** to predict Y in **testset** data and name your predictions as **knn_model5_predict**. What is the **RMSE** value in testset based on knn_model5_predict?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# HINT: use predict function\n# HINT: you canuse rmse() function in Metrics package\n# Enter your code below\n\n# predict Y in testset using knn_model5\nknn_model5_predict <- predict(knn_model5, newdata = testset)\n\n# calculate RMSE value in testset\nRMSE <- sqrt(mean((testset$Y - knn_model5_predict)^2))\n\n# print RMSE value\nRMSE\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.04109532\n```\n:::\n:::\n\n\n#### PART III Question 7\n\nIf we define best model as the one with lowest RMSE value in \\*\\***testset**\\*\\*, which of the following is your best model?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# HINT: use predict() function to get the predictions on testset\n# HINT: you canuse rmse() function in Metrics package to calculate RMSE values\n# Enter your code below\n\n# Predictions for knn_model1\nknn_model1_predict <- predict(knn_model1, newdata = testset)\nrmse(testset$Y, knn_model1_predict)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.03846585\n```\n:::\n\n```{.r .cell-code}\n# Predictions for knn_model5\nknn_model5_predict <- predict(knn_model5, newdata = testset)\nrmse(testset$Y, knn_model5_predict)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.04109532\n```\n:::\n\n```{.r .cell-code}\n# Predictions for model_forward\nmodel_forward_predict <- predict(model_forward, newdata = testset)\nrmse(testset$Y,model_forward_predict )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.03761071\n```\n:::\n\n```{.r .cell-code}\n# Predictions for model_backward\nmodel_backward_predict <- predict(model_backward, newdata = testset)\nrmse(testset$Y,model_backward_predict )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.03615816\n```\n:::\n\n```{.r .cell-code}\n#library(Metrics)\nRMSE_knn1 <- rmse(predict(knn_model1, newdata = testset), testset$Y)\nRMSE_knn5 <- rmse(knn_model5_predict, testset$Y)\nRMSE_backward <- rmse(model_backward_predict, testset$Y )\nRMSE_forward <- rmse(model_forward_predict, testset$Y )\n\n# print(paste(\"The value of my object is\", my_object))\nprint(paste(\"The value of RMSE_knn1 = \", RMSE_knn1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"The value of RMSE_knn1 =  0.0384658469125304\"\n```\n:::\n\n```{.r .cell-code}\nprint(paste(\"The value of RMSE_knn5 = \", RMSE_knn5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"The value of RMSE_knn5 =  0.0410953210194127\"\n```\n:::\n\n```{.r .cell-code}\nprint(paste(\"The value of RMSE_backward = \", RMSE_backward))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"The value of RMSE_backward =  0.036158161755491\"\n```\n:::\n\n```{.r .cell-code}\nprint(paste(\"The value of RMSE_forward = \", RMSE_forward))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"The value of RMSE_forward =  0.0376107105357471\"\n```\n:::\n:::\n\n\n#### Question 8\n\nClick on Render icon on to convert this file into HTML format before submitting in Canvas\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}